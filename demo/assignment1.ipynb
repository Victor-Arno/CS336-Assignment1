{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a9f9be",
   "metadata": {},
   "source": [
    "# 2.1 Unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51823c0",
   "metadata": {},
   "source": [
    "## <font color=\"red\">(a)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd3af06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b79bbc8",
   "metadata": {},
   "source": [
    "## <font color=\"red\">(b)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1a9f783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b96406cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = \"this is a test\" + chr(0) + \"string\"\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd3af6",
   "metadata": {},
   "source": [
    "## <font color=\"red\">(c)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b419c473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a4e42",
   "metadata": {},
   "source": [
    "# 2.2 Unicode  Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33837821",
   "metadata": {},
   "source": [
    "## <font color=\"red\">(a)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7ef10866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'h\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UTF-8编码\n",
    "\"hello\".encode(\"utf-8\")  # b'hello' - ASCII字符保持原样\n",
    "# 每个ASCII字符正好是1字节\n",
    "\n",
    "# UTF-16编码\n",
    "\"hello\".encode(\"utf-16-le\")  # b'h\\x00e\\x00l\\x00l\\x00o\\x00'\n",
    "# 每个ASCII字符变为2字节，其中一半是空字节(0x00)\n",
    "\n",
    "# UTF-32编码\n",
    "\"hello\".encode(\"utf-32-le\")  # b'h\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00'\n",
    "# 每个ASCII字符变为4字节，其中3/4是空字节"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe239b5",
   "metadata": {},
   "source": [
    "## <font color=\"red\">(b)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62184799",
   "metadata": {},
   "source": [
    "原因: 它逐个字节解码UTF-8，而UTF-8字符可能是多字节的.\n",
    "破坏了多字节序列.UTF-8多字节字符必须作为一个整体解码\n",
    ",拆分字节会得到完全不同的字符或无效序列.\n",
    "eg. \"你\" 的UTF-8编码是 b'\\xe4\\xbd\\xa0'\n",
    "如果逐个字节解码,会得到:\n",
    "b'\\xe4' -> 'ä'\n",
    "b'\\xbd' -> '½'\n",
    "b'\\xa0' -> ' '\n",
    "这不是一个有效的UTF-8字符序列,因此会被解码为替换字符(U+FFFD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "963130c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "string = \"hello\"\n",
    "decode_utf8_bytes_to_str_wrong(string.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10128bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xe4\\xbd\\xa0'\n",
      "你\n"
     ]
    }
   ],
   "source": [
    "print(\"你\".encode(\"utf-8\"))\n",
    "print(\"你\".encode(\"utf-8\").decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9bc0b57e",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\xe4\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "print(b'\\xe4'.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ebd76",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你好\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m, in \u001b[0;36mdecode_utf8_bytes_to_str_wrong\u001b[1;34m(bytestring)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "string = \"你\"\n",
    "decode_utf8_bytes_to_str_wrong(string.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450bb06b",
   "metadata": {},
   "source": [
    "## <font color=\"red\">(c)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b2e06",
   "metadata": {},
   "source": [
    "\"中\".encode(\"utf-8\")  # b'\\xe4\\xb8\\xad' 是有效的三字节序列\n",
    "<br>\n",
    "#但如果拆分：b'\\xe4\\xb8' 就只有两个字节，但缺少最后一个字节，也是无效的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c9534",
   "metadata": {},
   "source": [
    "# 2.4 Pre-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7152845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你好', '，', '这是一个测试字符串', '！']\n",
      "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "print(re.findall(PAT, \"你好，这是一个测试字符串！\"))\n",
    "print(re.findall(PAT, \"some text that i'll pre-tokenize\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95431d83",
   "metadata": {},
   "source": [
    "# 2.5 Experimenting with BPE Tokenizer Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c45dcfd",
   "metadata": {},
   "source": [
    "相比https://github.com/heng380/cs336-assignment1的改进"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0955d2b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 新tokenizer相比旧版本的主要改变和优化\n",
    "\n",
    "### 1. **注释和文档改进**\n",
    "- **所有注释中文化**：将英文注释全部替换为中文注释，便于中文用户理解\n",
    "- **增加详细文档**：为每个函数和类添加了详细的中文文档字符串\n",
    "\n",
    "### 2. **分块处理优化**\n",
    "- **换行符规范化**：在`get_chunk`函数中增加了换行符统一处理，将`\\r\\n`和`\\r`都转换为`\\n`\n",
    "- **分割标记更改**：将分块分割标记从`b\"<|endoftext|>\"`改为`b\"\\n\"`，更符合实际文本分割需求\n",
    "\n",
    "### 3. **核心算法性能优化**\n",
    "\n",
    "#### `count_pair_frequencies`方法优化\n",
    "```python\n",
    "# 旧版本：每次循环都计算len(word)\n",
    "for word, freq in tokens_counter.items():\n",
    "    for i in range(len(word) - 1):\n",
    "\n",
    "# 新版本：避免重复计算长度，提高性能\n",
    "for word, freq in tokens_counter.items():\n",
    "    word_len = len(word)  # 优化：避免重复计算长度\n",
    "    for i in range(word_len - 1):\n",
    "```\n",
    "\n",
    "#### `find_max_pair`方法重写\n",
    "- **旧版本**：使用`max(counts, key=lambda x: (counts[x], x))`\n",
    "- **新版本**：手动实现循环比较，避免lambda函数开销，并修复了字典序选择逻辑\n",
    "- **重要修复**：当频率相同时，选择字典序更大的pair（与原始代码一致）\n",
    "\n",
    "#### `merge_tokens`方法重大优化\n",
    "**主要改进：**\n",
    "- **原地更新**：不再创建新的Counter对象，而是原地更新现有的tokens_counter\n",
    "- **pair_counts维护**：新增pair_counts参数，在合并时实时更新字节对频率\n",
    "- **批量操作**：使用tokens_to_remove和tokens_to_add列表进行批量更新\n",
    "- **性能提升**：避免重复计算pair_counts，显著提高训练速度\n",
    "\n",
    "### 4. **训练过程优化**\n",
    "\n",
    "#### `train_BPE`方法改进\n",
    "- **进程数限制**：`num_processes = min(multiprocessing.cpu_count(), 8)`，避免过度并行化\n",
    "- **chunksize优化**：使用`chunksize=max(1, len(chunks) // num_processes)`提高并行效率\n",
    "- **pair_counts单次初始化**：在训练循环外初始化pair_counts，避免重复计算\n",
    "- **进度条优化**：提前计算目标词汇量，进度条显示更准确\n",
    "\n",
    "### 5. **Tokenizer类编码优化**\n",
    "- **移除tqdm进度条**：在`encode`方法中移除了tqdm进度条，提高编码性能\n",
    "- **保持功能完整性**：所有核心功能保持不变\n",
    "\n",
    "### 6. **代码结构和可读性改进**\n",
    "- **类和方法注释**：为所有类和方法添加了详细的中文注释\n",
    "- **变量命名更清晰**：如`tokens_to_remove`、`tokens_to_add`等变量名更直观\n",
    "- **代码逻辑分组**：使用中文注释对代码进行逻辑分组\n",
    "\n",
    "## 性能提升总结\n",
    "\n",
    "1. **训练速度提升**：通过pair_counts的单次初始化和原地更新，显著减少重复计算\n",
    "2. **内存效率提升**：避免创建不必要的Counter对象，减少内存分配\n",
    "3. **并行效率优化**：合理的进程数限制和chunksize设置\n",
    "4. **算法复杂度优化**：减少重复的长度计算和字典操作\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f98342",
   "metadata": {},
   "source": [
    "# 3.6 资源计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22c2f73",
   "metadata": {},
   "source": [
    "### 单个 Block 的 MHA FLOPs 计算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1779f631",
   "metadata": {},
   "source": [
    "| 操作 | FLOPs 公式 | 说明 |\n",
    "|------|------------|------|\n",
    "| Q 投影 | $2 \\times seq\\_len \\times d\\_model^2$ | 线性变换 |\n",
    "| K 投影 | $2 \\times seq\\_len \\times d\\_model^2$ | 线性变换 |\n",
    "| V 投影 | $2 \\times seq\\_len \\times d\\_model^2$ | 线性变换 |\n",
    "| Q @ Kᵀ | $2 \\times seq\\_len^2 \\times d\\_model$ | 注意力分数计算 |\n",
    "| Attn @ V | $2 \\times seq\\_len^2 \\times d\\_model$ | 注意力加权求和 |\n",
    "| Output 投影 | $2 \\times seq\\_len \\times d\\_model^2$ | 输出变换 |\n",
    "\n",
    "#### 总计公式\n",
    "$$\n",
    "\\text{FLOPs}_{MHA} = 8 \\times seq\\_len \\times d\\_model^2 + 4 \\times seq\\_len^2 \\times d\\_model\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24bb6f",
   "metadata": {},
   "source": [
    "### 单个 Block 的 FFN FLOPs 计算\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fab9da5",
   "metadata": {},
   "source": [
    "\n",
    "| 操作 | FLOPs 公式 | 说明 |\n",
    "|------|------------|------|\n",
    "| $xW_1$ | $2 \\times seq\\_len \\times d\\_model \\times d\\_ff$ | 第一层线性变换 |\n",
    "| $xW_3$ | $2 \\times seq\\_len \\times d\\_model \\times d\\_ff$ | 第二层线性变换（SwiGLU结构） |\n",
    "| $(\\cdot)W_2$ | $2 \\times seq\\_len \\times d\\_ff \\times d\\_model$ | 输出投影 |\n",
    "\n",
    "#### 总计公式\n",
    "\n",
    "$$\n",
    "\\text{FLOPs}_{FFN} = 6 \\times seq\\_len \\times d\\_model \\times d\\_ff\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9372b237",
   "metadata": {},
   "source": [
    "### 完整 Transformer FLOPs 计算\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dbc6d8",
   "metadata": {},
   "source": [
    "#### 单个 Transformer Block FLOPs\n",
    "\n",
    "#### MHA 部分\n",
    "$$\n",
    "\\text{FLOPs}_{MHA} = 8 \\times seq\\_len \\times d\\_model^2 + 4 \\times seq\\_len^2 \\times d\\_model\n",
    "$$\n",
    "\n",
    "#### FFN 部分\n",
    "$$\n",
    "\\text{FLOPs}_{FFN} = 6 \\times seq\\_len \\times d\\_model \\times d\\_ff\n",
    "$$\n",
    "\n",
    "#### 单个 Block 总计\n",
    "$$\n",
    "\\text{FLOPs}_{block} = 8 \\times seq\\_len \\times d\\_model^2 + 4 \\times seq\\_len^2 \\times d\\_model + 6 \\times seq\\_len \\times d\\_model \\times d\\_ff\n",
    "$$\n",
    "\n",
    "#### 所有 Transformer Blocks（48层）\n",
    "$$\n",
    "\\text{FLOPs}_{blocks} = 48 \\times (8 \\times seq\\_len \\times d\\_model^2 + 4 \\times seq\\_len^2 \\times d\\_model + 6 \\times seq\\_len \\times d\\_model \\times d\\_ff)\n",
    "$$\n",
    "\n",
    "#### Output Linear 层\n",
    "$$\n",
    "\\text{FLOPs}_{output} = 2 \\times seq\\_len \\times d\\_model \\times vocab\\_size\n",
    "$$\n",
    "\n",
    "#### 总 FLOPs\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{FLOPs}_{total} &= 48 \\times (8 \\times seq\\_len \\times d\\_model^2 + 4 \\times seq\\_len^2 \\times d\\_model + 6 \\times seq\\_len \\times d\\_model \\times d\\_ff) \\\\\n",
    "&\\quad + 2 \\times seq\\_len \\times d\\_model \\times vocab\\_size\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddeca0c",
   "metadata": {},
   "source": [
    "## (a) 参数量和内存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d295c455",
   "metadata": {},
   "source": [
    "### GPT-2 XL 参数量计算\n",
    "\n",
    "### 各模块参数量\n",
    "\n",
    "| 模块 | 参数量计算公式 | 参数数量 |\n",
    "|------|--------------|----------|\n",
    "| Token Embedding | $vocab\\_size \\times d\\_model$ | $50257 \\times 1600 = 80,411,200$ |\n",
    "| 每个 Block 的 MHA | $4 \\times d\\_model^2$ | $4 \\times 1600^2 = 10,240,000$ |\n",
    "| 每个 Block 的 FFN | $3 \\times d\\_model \\times d\\_ff$ | $3 \\times 1600 \\times 6400 = 30,720,000$ |\n",
    "| 每个 Block 的 RMSNorm (×2) | $2 \\times d\\_model$ | $2 \\times 1600 = 3,200$ |\n",
    "| Final RMSNorm | $d\\_model$ | $1,600$ |\n",
    "| Output Linear | $d\\_model \\times vocab\\_size$ | $1600 \\times 50257 = 80,411,200$ |\n",
    "\n",
    "### 总参数量计算\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{总参数量} &= \\text{Token Embedding} + 48 \\times (\\text{MHA} + \\text{FFN} + \\text{RMSNorm}) + \\text{Final RMSNorm} + \\text{Output Linear} \\\\\n",
    "&= 80,411,200 + 48 \\times (10,240,000 + 30,720,000 + 3,200) + 1,600 + 80,411,200 \\\\\n",
    "&\\approx \\textbf{1.56B 参数}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 内存占用\n",
    "\n",
    "- 单精度 (float32)：每个参数 4 bytes\n",
    "- 总内存：$1.56 \\times 10^9 \\times 4 \\text{ bytes} \\approx \\textbf{6.24 GB}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a998b",
   "metadata": {},
   "source": [
    "## (b) 矩阵乘法列表和总 FLOPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0d505",
   "metadata": {},
   "source": [
    "\n",
    "| 组件 | 矩阵乘法 | FLOPs |\n",
    "|------|----------|-------|\n",
    "| MHA: Q 投影 | $(seq, d\\_model) \\times (d\\_model, d\\_model)$ | $2 \\times 1024 \\times 1600^2$ |\n",
    "| MHA: K 投影 | 同上 | $2 \\times 1024 \\times 1600^2$ |\n",
    "| MHA: V 投影 | 同上 | $2 \\times 1024 \\times 1600^2$ |\n",
    "| MHA: Q @ K^T | $(seq, d\\_head) \\times (d\\_head, seq) \\times num\\_heads$ | $2 \\times 1024^2 \\times 1600$ |\n",
    "| MHA: Attn @ V | $(seq, seq) \\times (seq, d\\_head) \\times num\\_heads$ | $2 \\times 1024^2 \\times 1600$ |\n",
    "| MHA: Output 投影 | $(seq, d\\_model) \\times (d\\_model, d\\_model)$ | $2 \\times 1024 \\times 1600^2$ |\n",
    "| FFN: xW1 | $(seq, d\\_model) \\times (d\\_model, d\\_ff)$ | $2 \\times 1024 \\times 1600 \\times 6400$ |\n",
    "| FFN: xW3 | 同上 | $2 \\times 1024 \\times 1600 \\times 6400$ |\n",
    "| FFN: ·W2 | $(seq, d\\_ff) \\times (d\\_ff, d\\_model)$ | $2 \\times 1024 \\times 6400 \\times 1600$ |\n",
    "| Output Linear | $(seq, d\\_model) \\times (d\\_model, vocab)$ | $2 \\times 1024 \\times 1600 \\times 50257$ |\n",
    "\n",
    "### 总 FLOPs\n",
    "\n",
    "$$\n",
    "48 \\times (8 \\times 1024 \\times 1600^2 + 4 \\times 1024^2 \\times 1600 + 6 \\times 1024 \\times 1600 \\times 6400) + 2 \\times 1024 \\times 1600 \\times 50257\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\approx \\textbf{1.83 TFLOPs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a7f41d",
   "metadata": {},
   "source": [
    "## (c) 哪部分消耗最多 FLOPs？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66969e43",
   "metadata": {},
   "source": [
    "根据计算分析，GPT-2 XL 在推理过程中各模块的FLOPs分布如下：\n",
    "\n",
    "| 组件模块 | FLOPs 占比 | 详细说明 |\n",
    "|----------|------------|----------|\n",
    "| **FFN 层** | **~54%** | 占比最高，主要由三个大矩阵乘法组成：<br>• $2 \\times 1024 \\times 1600 \\times 6400$ (xW1)<br>• $2 \\times 1024 \\times 1600 \\times 6400$ (xW3)<br>• $2 \\times 1024 \\times 6400 \\times 1600$ (·W2) |\n",
    "| **MHA 投影层** | **~28%** | 包括 Q、K、V 输入投影和输出投影：<br>• $4 \\times (2 \\times 1024 \\times 1600^2)$ |\n",
    "| **Attention 计算** | **~12%** | 注意力机制的核心计算：<br>• $2 \\times 1024^2 \\times 1600$ (Q@Kᵀ)<br>• $2 \\times 1024^2 \\times 1600$ (Attn@V) |\n",
    "| **Output Linear** | **~6%** | 最后的输出层：<br>• $2 \\times 1024 \\times 1600 \\times 50257$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d15aef",
   "metadata": {},
   "source": [
    "## (d) 不同 GPT-2 模型的对比\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b93bd5",
   "metadata": {},
   "source": [
    "### GPT-2 系列模型 FLOPs 与计算分布分析\n",
    "\n",
    "| 模型 | 层数 | d_model | heads | d_ff | 总 FLOPs | FFN 占比 | Attention 占比 |\n",
    "|------|------|---------|-------|------|----------|----------|----------------|\n",
    "| GPT-2 Small | 12 | 768 | 12 | 3072 | ~109 GFLOPs | ~54% | ~13% |\n",
    "| GPT-2 Medium | 24 | 1024 | 16 | 4096 | ~366 GFLOPs | ~54% | ~13% |\n",
    "| GPT-2 Large | 36 | 1280 | 20 | 5120 | ~838 GFLOPs | ~54% | ~12% |\n",
    "| GPT-2 XL | 48 | 1600 | 25 | 6400 | ~1.83 TFLOPs | ~54% | ~12% |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee2f26",
   "metadata": {},
   "source": [
    "## (e)：context_length 增加到 16,384\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ceacb4",
   "metadata": {},
   "source": [
    "\n",
    "### GPT-2 XL 模型在 seq_len = 16,384 时的 FLOPs 计算\n",
    "\n",
    "#### 计算公式\n",
    "$$\n",
    "\\text{FLOPs} = 48 \\times (8 \\times 16384 \\times 1600^2 + 4 \\times 16384^2 \\times 1600 + 6 \\times 16384 \\times 1600 \\times 6400) + 2 \\times 16384 \\times 1600 \\times 50257\n",
    "$$\n",
    "\n",
    "#### 计算结果\n",
    "$$\n",
    "\\approx \\textbf{84.5 × 10¹² FLOPs (84.5 TFLOPs)}\n",
    "$$\n",
    "\n",
    "#### 对比分析\n",
    "相比原来的 4.35 TFLOPs，增加了约 **19.4 倍**。\n",
    "\n",
    "#### 组件占比变化\n",
    "\n",
    "| 组件 | seq=1024 占比 | seq=16384 占比 |\n",
    "|------|---------------|----------------|\n",
    "| MHA 投影 | 23.1% | 14.7% |\n",
    "| Attention (Q@K^T + Attn@V) | 4.1% | 40.3% |\n",
    "| FFN | 72.4% | 44.7% |\n",
    "| Output | 0.4% | 0.3% |\n",
    "\n",
    "#### 结论\n",
    "\n",
    "当 context_length 增加 16 倍时，**Attention 计算变成主要瓶颈**（从 4% 增加到 40%）。这是因为 Attention 的复杂度是 $O(\\text{seq}^2)$，而其他组件是 $O(\\text{seq})$。\n",
    "\n",
    "这就是为什么长序列模型需要各种 **Efficient Attention 方法**（如 Flash Attention、Sparse Attention 等）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9971772",
   "metadata": {},
   "source": [
    "第二步：展开公式\n",
    "Softmax 的完整形式：\n",
    "$$\\text{softmax}(o)[k] = \\frac{\\exp(o[k])}{\\sum_{j} \\exp(o[j])}$$\n",
    "\n",
    "代入 $-\\log$：\n",
    "$$-\\log \\text{softmax}(o)[k] = -\\log \\frac{\\exp(o[k])}{\\sum_{j} \\exp(o[j])}$$\n",
    "\n",
    "用 $\\log \\frac{a}{b} = \\log a - \\log b$：\n",
    "$$= -\\left(\\log \\exp(o[k]) - \\log \\sum_{j} \\exp(o[j])\\right)$$\n",
    "\n",
    "$$= -o[k] + \\log \\sum_{j} \\exp(o[j])$$\n",
    "\n",
    "最终公式：\n",
    "$$\\ell = -o[target] + \\log \\sum_{j} \\exp(o[j])$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "数值稳定性推导\n",
    "设 $m = \\max(o)$\n",
    "\n",
    "$$\\log \\sum_j \\exp(o[j] - m) = \\log \\sum_j \\frac{\\exp(o[j])}{\\exp(m)}$$\n",
    "\n",
    "$$= \\log \\frac{\\sum_j \\exp(o[j])}{\\exp(m)}$$\n",
    "\n",
    "$$= \\log \\sum_j \\exp(o[j]) - \\log \\exp(m)$$\n",
    "\n",
    "$$= \\log \\sum_j \\exp(o[j]) - m$$\n",
    "\n",
    "所以：\n",
    "$$\\log \\sum_j \\exp(o[j]) = m + \\log \\sum_j \\exp(o[j] - m)$$\n",
    "\n",
    "第四步：最终稳定的公式\n",
    "把这个代入我们的 cross entropy：\n",
    "\n",
    "$$\\ell = -o[target] + \\log \\sum_{j} \\exp(o[j])$$\n",
    "\n",
    "$$= -o[target] + m + \\log \\sum_{j} \\exp(o[j] - m)$$\n",
    "\n",
    "也可以写成：\n",
    "\n",
    "$$= -(o[target] - m) + \\log \\sum_{j} \\exp(o[j] - m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c44c3",
   "metadata": {},
   "source": [
    "# 4.2 学习率的探究"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a49e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\VictorArno\\AppData\\Local\\Temp\\ipykernel_18760\\2719549927.py\", line 3, in <module>\n",
      "    import torch\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "d:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "from collections.abc import Callable, Iterable\n",
    "from typing import Optional\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        defaults = {\"lr\": lr}\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        loss = None if closure is None else closure()  # 兼容API（可选：重新计算loss）\n",
    "        # 遍历每个参数组（param_groups）\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]  # 获取当前组的学习率\n",
    "            \n",
    "            # 遍历组内每个参数\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:  # 如果参数没有梯度，跳过\n",
    "                    continue\n",
    "                \n",
    "                # 获取参数的“状态（state）”：存储迭代次数t\n",
    "                state = self.state[p]\n",
    "                t = state.get(\"t\", 0)  # 初始t=0，之后每次迭代+1\n",
    "                \n",
    "                grad = p.grad.data  # 获取参数的梯度\n",
    "                # 按照公式更新参数（in-place修改p.data）\n",
    "                p.data -= lr / math.sqrt(t + 1) * grad\n",
    "                \n",
    "                state[\"t\"] = t + 1  # 迭代次数+1\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a1ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.625673294067383\n",
      "28.452499389648438\n",
      "27.653432846069336\n",
      "27.01848793029785\n",
      "26.480819702148438\n",
      "26.009235382080078\n",
      "25.58624267578125\n",
      "25.200876235961914\n",
      "24.845741271972656\n",
      "24.51556968688965\n",
      "24.206449508666992\n",
      "23.915390014648438\n",
      "23.640037536621094\n",
      "23.378498077392578\n",
      "23.129240036010742\n",
      "22.890979766845703\n",
      "22.662641525268555\n",
      "22.443317413330078\n",
      "22.232219696044922\n",
      "22.028667449951172\n",
      "21.832077026367188\n",
      "21.641929626464844\n",
      "21.457761764526367\n",
      "21.279159545898438\n",
      "21.10577392578125\n",
      "20.937265396118164\n",
      "20.773340225219727\n",
      "20.61373519897461\n",
      "20.45820426940918\n",
      "20.30652618408203\n",
      "20.15850067138672\n",
      "20.01393699645996\n",
      "19.87266731262207\n",
      "19.734533309936523\n",
      "19.599388122558594\n",
      "19.46709632873535\n",
      "19.3375301361084\n",
      "19.2105770111084\n",
      "19.086122512817383\n",
      "18.964073181152344\n",
      "18.844322204589844\n",
      "18.72678565979004\n",
      "18.61138153076172\n",
      "18.498023986816406\n",
      "18.386646270751953\n",
      "18.277172088623047\n",
      "18.169536590576172\n",
      "18.06368064880371\n",
      "17.95953941345215\n",
      "17.8570613861084\n",
      "17.756189346313477\n",
      "17.656871795654297\n",
      "17.559066772460938\n",
      "17.462722778320312\n",
      "17.3677978515625\n",
      "17.274246215820312\n",
      "17.182035446166992\n",
      "17.09112548828125\n",
      "17.001474380493164\n",
      "16.913053512573242\n",
      "16.825828552246094\n",
      "16.73976707458496\n",
      "16.654834747314453\n",
      "16.571008682250977\n",
      "16.488256454467773\n",
      "16.40655517578125\n",
      "16.32587242126465\n",
      "16.24618911743164\n",
      "16.16748046875\n",
      "16.089719772338867\n",
      "16.012887954711914\n",
      "15.93696403503418\n",
      "15.861924171447754\n",
      "15.787750244140625\n",
      "15.714425086975098\n",
      "15.641927719116211\n",
      "15.570239067077637\n",
      "15.499342918395996\n",
      "15.429224967956543\n",
      "15.359867095947266\n",
      "15.291251182556152\n",
      "15.223367691040039\n",
      "15.156196594238281\n",
      "15.08972454071045\n",
      "15.023938179016113\n",
      "14.958827018737793\n",
      "14.89437484741211\n",
      "14.830570220947266\n",
      "14.7673978805542\n",
      "14.704852104187012\n",
      "14.642914772033691\n",
      "14.581579208374023\n",
      "14.520834922790527\n",
      "14.460667610168457\n",
      "14.401066780090332\n",
      "14.342028617858887\n",
      "14.283535957336426\n",
      "14.22558307647705\n",
      "14.168161392211914\n",
      "14.111261367797852\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 实验：测试不同学习率对训练的影响\n",
    "def experiment_learning_rates():\n",
    "    learning_rates = [1e1, 1e2, 1e3]  # 10, 100, 1000\n",
    "    num_iterations = 10\n",
    "    \n",
    "    results = {}  # 存储每个学习率的loss历史\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\n学习率 = {lr}\")\n",
    "        \n",
    "        # 1. 初始化参数\n",
    "        x = torch.tensor([10.0], requires_grad=True)\n",
    "        \n",
    "        # 2. 创建优化器\n",
    "        optimizer = SGD([x], lr=lr)\n",
    "        \n",
    "        # 3. 记录loss历史\n",
    "        loss_history = []\n",
    "        \n",
    "        # 4. 训练10步\n",
    "        for i in range(num_iterations):\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 计算损失：f(x) = x^2\n",
    "            loss = x ** 2\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            \n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 记录loss\n",
    "            loss_value = loss.item()\n",
    "            loss_history.append(loss_value)\n",
    "            \n",
    "            print(f\"  Iteration {i}: x = {x.item():.6f}, loss = {loss_value:.6f}\")\n",
    "        \n",
    "        results[lr] = loss_history\n",
    "    \n",
    "    # 可视化结果\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for lr, loss_history in results.items():\n",
    "        plt.plot(range(num_iterations), loss_history, marker='o', label=f'lr={lr}')\n",
    "    \n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Rate Effect on Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log')  # 使用对数坐标，方便观察\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行实验\n",
    "results = experiment_learning_rates()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
